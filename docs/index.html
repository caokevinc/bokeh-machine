<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 800px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Lato', sans-serif;
    color: #121212;
    font-size: 16px;
    line-height: 1.5;
  }
  h1, h2, h3, h4 {
    font-family: 'Lato', sans-serif;
    text-align: left;
  }
  h1 {
    color: #002f4f;
    font-size: 30px;
    text-align: center;
    line-height: 1;
  }
  h2 {
    color: #2a5e82;
    font-size: 24px;
    margin-top: 40px;
  }
  h3 {
    color: #95b4c9;
    font-size: 20px;
    margin-top: 30px;
  }
  h4 {
    color: #3b3b3b;
    font-size: 16px;
    font-weight: 400;
    margin-top: 30px;
  }
  pre {
    padding: 0.75em 1em;
    background: #f4f4f4;
    border: solid 1px #eeeeee;
    border-radius: 4px;
    margin-bottom: 20px;
  }
  code {
    font-size: 85%;
    font-family: Monaco, monospace;
    color: #666666;
  }
  figcaption {
    color: #888888;
    font-size: 12px;
    font-style: italic;
    margin-top: 10px;
    margin-bottom: 10px;
    text-align: center;
  }
  img {
    display: block;
    margin-top: 10px;
    margin-left: auto;
    margin-right: auto;
  }
  table video {
    margin: 0;
  }
  video {
    margin-top: 10px;
    margin-bottom: 20px;
  }
  #bird-video {
    width: 400px;
    margin-top: 10px;
    margin-bottom: -6px;
  }
</style>
<title>CS 184 Final Project</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Lato:300,300i,400,400i,700,700i&display=swap" rel="stylesheet">
</head>

<body>
<h1>CS 184 Final Project Writeup</h1>
<h1>Bokeh Machine - Depth of Field Synthesizer</h1>
<h1>Kevin Cao, Angela Dong, Aniruddha Nrusimha </h1>

<h2>Abstract</h2>
<p>
  We were motivated by depth-of-field based visual effects like portrait mode (blurring based on depth) and Ken Burns (zooming based on depth). Given a single image, these effects can be implemented with the help of a full depth map, which is a map from each pixel to a scalar indicating its depth value. Our main technical objective for this project was to calculate a depth map for an image from scratch without using stereo or machine learning.
</p>
<p>Examples of visual effects:</p>
<div align="center">
  <table>
    <tr valign="top">
      <td >
        <img src="final-images/portrait.jpeg" width="375px" />
        <figcaption >Portrait mode: farther objects are blurred more heavily</figcaption>
      </td>
      <td >
        <img src="final-images/kenburns.gif" width="400px" />
        <figcaption >Ken Burns effect: closer objects zoom in more quickly</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>Example of a depth map:</p>
<img src="final-images/depthmap.jpg" width="700px" />
<figcaption >Sample depth map, with depth values ranging from 0 to 255; lighter color indicates closer</figcaption>
<p>
  After trying a few possible approaches, we decided to move forward with implementing an algorithm from the paper "Defocus map estimation from a single image" (3). We were able to achieve visually similar depth maps to both the results from the original paper and depth maps generated from photography apps. Once our implementation was finished, we compared it to an off-the-shelf depth map and applied the Ken Burns effect to our own images, with some limitations.
</p>

<h2>Technical Approach</h2>
<p>
We found a lot of literature about depth estimation. Intuitively, most methods
take advantage of the fact that the further something is away from the plane of
focus, the more out of focus it will be. These methods usually involve comparing
gradients at different regions of an image to determine how relatively blurry
those sections are.
</p>
<p>
  We first tried a method from the paper "Recovering Depth Map from Enhanced Image Gradients" (2)
  that tries to find a depth map with gradients close to the original image gradient.
  The assumption is that the gradients in the depth map should match up
  to the gradients in the image. The paper suggests an error function
  relating the image gradient to the depth map gradients and provides a
  closed form solution for the depth map.
</p>
<div align="center">
  <table>
    <tr valign="top">
      <td >
        <img src="milestone-images/enhancedalgo.png" width="800px" />
        <figcaption >Algorithm for constructing Depth Map from Bala et. al</figcaption>
      </td>
  </table>
  <table>
    <tr valign="top">
      <td >
        <img src="milestone-images/yq.png" width="400px" />
        <figcaption >Original</figcaption>
      </td>
      <td >
        <img src="milestone-images/enhanced.png" width="400px" />
        <figcaption >Depth Map</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
  However we found that this method gave us a weak, and frankly horrifying
  depth map. This, at least in its current implementation, is a dead end for us.
</p>
<h3>Defocus Map Estimation: Overview</h3>
<img src="milestone-images/edgeblur.png" width="800px" />
<figcaption >Overview of blur estimation approach, taken from Zhuo et. al</figcaption>
<p>
  This algorithm from "Defocus map estimation from a single image" was the second approach we tried and the one that worked best for us.
    In this approach, we blur the image and take the ratio of the gradients
    of the original image with the gradients of the blurred image. This ratio
    gives us an estimation of the standard deviation of the original blur of a
    region of the image, as we model out of focus areas as a Gaussian blur.
    This effectively gives us a "defocus map" which we can roughly equate to a
    depth map.
</p>
<p>
  Intuitively, this algorithm takes advantage of 2 assumptions: first, in an image with shallow depth of field, objects at a greater depth will be blurrier. Second, blurring an in-focus object will change its appearance more than blurring an already out-of-focus object. Overall, the ratio of gradients at a certain pixel before and after it has been blurred will correspond to how blurry, and therefore how far away from the camera the scene is at that pixel.
</p>
<img src="images/method/overview.png" width="800px" />
<figcaption >Method overview</figcaption>
<h4>Step 1: Gaussian blur the image</h4>
<p>
  We create two images:<br>
  i_1: original image with GaussianBlur of Standard Deviation 1 <br>
  i_2: original image with GaussianBlur of Standard Deviation 1.5 <br>
</p>
<div align="center">
  <table>
    <tr valign="top">
      <td >
        <img src="images/method/flower.png" width="200px" />
        <figcaption >Original photo</figcaption>
      </td>
      <td >
        <img src="images/method/og.png" width="200px" />
        <figcaption >i_1, grayed and blurred with &sigma; = 1</figcaption>
      </td>
      <td >
        <img src="images/method/blurry.png" width="200px" />
        <figcaption >i_2, grayed and blurred with &sigma; = 1.5</figcaption>
      </td>
    <tr>
    </table>
  </div>
<h4>Step 2: Take gradients of i_1 and i_2</h4>
<p>
  We use Sobel filters of kernel size 1 to take image gradients in the x and y
  directions to compute the magnitude of the full gradient.
</p>
<div align="center">
  <table>
    <tr valign="top">
      <td >
        <img src="images/method/gradienteq.png" width="320px" />
        <figcaption >Magnitude of the gradient, where i_x and i_y are gradients in the x and y direction</figcaption>
      </td>
    <tr>
    </table>
  </div>
Python implementation:
<pre><code>sobelx = cv2.Sobel(im, cv2.CV_64F, 1, 0, ksize=1)
sobely = cv2.Sobel(im, cv2.CV_64F, 0, 1, ksize=1)
grad = np.sqrt(sobelx**2 + sobely**2)</code></pre>
<p>
We use this to compute the gradients, gradient(i_1) and gradient(i_2).
</p>
<div align="center">
  <table>
    <tr valign="top">
      <td >
        <img src="images/method/grad1.png" width="320px" />
        <figcaption >Gradient of i_1</figcaption>
      </td>
      <td >
        <img src="images/method/grad2.png" width="320px" />
        <figcaption >Gradient of i_2</figcaption>
      </td>
    </tr>
  </table>
</div>



<h4>Step 3: Use ratio of gradients to derive depth estimation</h4>
<p>We take the ratio of gradients R = gradient(i_1)/gradient(i_2).</p>

<div align="center">
  <table>
    <tr valign="top">
      <td >
        <img src="images/method/diff.png" width="320px" />
        <figcaption >Map of Gradients</figcaption>
      </td>
    </tr>
    </table>
  </div>

  Derivation of depth estimation:

  <div align="center">
    <table>
      <tr valign="top">
        <td >
          <img src="images/method/r2.png" width="150px" />
          <figcaption ></figcaption>
        </td>
      </tr>
    </table>
  </div>
  <p>
    where:<br/>
    R is ratio of gradients<br>
    s is scene depth<br>
    &sigma;_1 is standard deviation of Gaussian blur applied to i_1<br>
    &sigma;_2 is standard deviation of Gaussian blur applied to i_2<br>
  </p><p>
    The intuition is that at infinite depth, or a completely out of focus region
    of the image, adding a gaussian blur does nothing, aka R = 1. At 0 depth, or
    a completely sharp region of the image, we expect R = &sigma;_1/&sigma;_2; in
    other words, only the Gaussian blurs have caused the difference. With some
    algebra we are able to derive s, the estimated depth.
  </p>
    <div align="center">
      <table>
        <tr valign="top">
          <td >
            <img src="images/method/s.png" width="150px" />
            <figcaption ></figcaption>
          </td>
        </tr>
      </table>
    </div>
    <div align="center">
      <table>
        <tr valign="top">
          <td >
            <img src="images/method/sig.png" width="320px" />
            <figcaption >Estimated depth map</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <p>
      We notice that the depth estimation is strongest at edges. Defocus
      estimation is only useful at edges, since the blur affects edges high
      frequency areas the most.
    </p>

    <h4>Step 4: Use depth map and edge map to create sparse defocus map</h4>

    <p>We use a Canny edge detector to mask for only the edges in our depth
      estimation map. This produces a sparse defocus map, where we have an
      estimate of depth at every edge.
    </p>
    <div align="center">
      <table>
        <tr valign="top">
          <td >
            <img src="images/method/sig.png" width="200px" />
            <figcaption >Estimated depth map</figcaption>
          </td>
          <td >
            <img src="images/method/star.png" width="60px" />
          </td>
          <td >
            <img src="images/method/edgemap.png" width="200px" />
            <figcaption >Edge map</figcaption>
          </td>
          <td >
            <img src="images/method/equal.png" width="55px" />
          </td>
          <td >
            <img src="images/method/sparse.png" width="200px" />
            <figcaption >Sparse defocus map</figcaption>
          </td>
        </tr>
      </table>
    </div>
<h4>Step 5: Matting Laplacian - Interpolating from the Sparse Defocus Map</h4>

<p>Finally, we calculate the matting Laplacian matrix of the original image. An image's matting Laplacian is a H*W x H*W matrix that quantifies how different each pair of pixels is from one another, taking both RGB value and pixel distance into account. This is useful in segmentation tasks like calculating the alpha matte, a mask that segments foreground from background, of a given image.</p>
<img src="images/method/laplacian.png" width="500px" />
<figcaption >Equation of matting Laplacian</figcaption>
<p>We can use this measure of pairwise difference to determine which pixels are likely to belong to the same object and therefore the same depth level. This allows us to interpolate between depths in our sparse edge map to fill it in.</p>
<p>Mathematically, we want to find a full defocus map d, which should both be close to the sparse defocus map d-hat everywhere d-hat is defined, and change continuity at image edges; we can formulate this as the minimization of a cost function E(d):</p>
<img src="images/method/e.png" width="320px" />
<figcaption >Full depth map cost function</figcaption>
<p>We can derive that argmin(d) satisfies the following equation:</p>
<img src="images/method/l.png" width="180px" />
<figcaption >Linear equation that holds at optimal d</figcaption>
where:<br/>
d-hat is the sparse defocus map we have<br/>
d is the full defocus map we want<br/>
&lambda; is a parameter balancing smoothness vs. fidelity to our sparse map<br/>
L is the image's matting Laplacian<br/>
D is a diagonal representation of our earlier edge detection<br/>
<p>We implement this in Python using cf_laplacian from the pymatting library and scipy's spsolve linear equation solver.</p>
<p>Matting Laplacian and edge diagonal:</p>
<pre><code>normed = (image.astype(float) / np.linalg.norm(image.astype(float)))
L = cf_laplacian(normed, epsilon=1e-11, radius=1)
D = scipy.sparse.diags(edges.flatten(), dtype=np.int8)</code></pre>
Setting up and solving the linear equation:
<pre><code>lamb = 0.001
A = scipy.sparse.csc_matrix(L + lamb * D)
b = scipy.sparse.csc_matrix(lamb * D * sparse_map.flatten()).T
x = scipy.sparse.linalg.spsolve(A, b)[:, np.newaxis]
full_map = x.reshape(sparse.shape)</code></pre>
Now, the variable full_map contains d, the full depth map.
<h4>Step 6: (Optional) Increase contrast</h4>
<p>
  Sometimes we get flat depth maps. To increase the spread of depth,
  we can use a gamma correction to increase the contrast.
</p>
<div align="center">
  <table>
    <tr valign="top">
      <td >
        <img src="images/method/gamma.png" width="100px" />
        <figcaption >Gamma correction</figcaption>
      </td>
    </tr>
  </table>
  <table>
    <tr valign="top">
      <td >
        <img src="images/method/iflower.png" width="320px" />
        <figcaption >Raw defocus map</figcaption>
      </td>
      <td >
        <img src="images/method/iflowercontrast.png" width="320px" />
        <figcaption >Added contrast defocus map</figcaption>
      </td>
    </tr>
  </table>
</div>



<p> Using the constructed depth map, we get strong results on a variety of images.
   As reference, lighter portions of the image are closer to the camera </p>
<p> These results also allow us to construct visual effects on our results </p>
<div align="center">
  <table>
    <tr>
      <td>
        <img src="images/input.png" width="250px"/>
        <figcaption>Original image</figcaption>
      </td>
      <td>
        <img src="images/bird_depth.png" width="250px"/>
        <figcaption>Depth we generated</figcaption>
      </td>
      <td>
        <img src="images/binary_blur.png" width="250px"/>
        <figcaption>Foreground - background extraction</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>We can use these depth extractions for cool effects, which we will discuss later.</p>
<div align="center">
  <table>
    <tr>
      <td>
        <img src="images/super_blur.png" width="400px"/>
        <figcaption>Enhancing bokeh effects for the picture</figcaption>
      </td>
      <td>
        <video id="bird-video" controls>
          <source src="images/bird.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <figcaption>Ken Burns effect</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>We tested our depth map estimation on a varied of images, and got variable results.
  The method seems to work best on scenes with very clear foreground and background
  separation.
  We collected a ground truth using the Halide Iphone app, which has a depth map
  feature using the stereo camera system. </p>

<div align="center">
  <table>
    <tr valign="top">
      <td>
        <img src="images/results/photo1.jpeg" width="320px"/>
        <figcaption>First picture of flowers</figcaption>
      </td>
      <td>
        <img src="images/results/iphoto1.png" width="320px"/>
        <figcaption>Our Method</figcaption>
      </td>
      <td>
        <img src="images/results/photo1d.png" width="320px"/>
        <figcaption>Ground Truth<figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/results/photo2.jpeg" width="320px"/>
        <figcaption>Second pictures of flowers</figcaption>
      </td>
      <td>
        <img src="images/results/iphoto2.png" width="320px"/>
        <figcaption>Our Method</figcaption>
      </td>
      <td>
        <img src="images/results/photo2d.png" width="320px"/>
        <figcaption>Ground Truth</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/results/photo3.jpeg" width="320px"/>
        <figcaption>Third picture of flowers</figcaption>
      </td>
      <td>
        <img src="images/results/iphoto3.png" width="320px"/>
        <figcaption>Our Method</figcaption>
      </td>
      <td>
        <img src="images/results/photo3d.png" width="320px"/>
        <figcaption>Ground Truth</figcaption>
      </td>
    </tr>
  </table>
</div>

<h2>Results</h2>
<h3>Out of the box depth estimation and effects </h3>
<p> We also used out of the box depth estimation from the Ken Burns paper
   to test out some cool effects. We listed two that we implemented below. </p>
<div align="center">
  <table>
    <tr>
      <td>
        <img src="images/rclouds.png" width="400px"/>
        <figcaption>Original Image</figcaption>
      </td>
      <td>
        <img src="images/depth.jpg" width="400px"/>
        <figcaption>Depth mapping</figcaption>
      </td>
    </tr>
  </table>
  <table>
    <tr>
      <td>
        <b>3D Ken Burns Effect</b>
      </td>
      <td>
        Elements which are closer to the camera are zoomed in more than those farther away, simulating the camera moving closer to the image.
        In addition, blown up closer versions of the image are laid over the images farther back.  We mostly used an out of the box implementation for this part.
      </td>
      <td>
        <video width="400px" controls>
          <source src="images/autozoom.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>
    </tr>
    <tr>
      <td>
        <b>Artificial Lens Blur</b>
      </td>
      <td>
        We establish a focus point in the depth map.  Because we only had relative depth measurements for depth, we assumed the image was focused at infinity.
        We then simulated a thin lens effect by picking a point as our point of focus, and estimating the degree of blur using lens equations learned in class.
        This technique is meant for cameras focused at infinity, and is effectively the 'inverse' of the idea of the defocus map - given a depth map, determine blur.
      </td>
      <td>
        <img src="images/portrait.png" width="400px"/>
        <figcaption>partially blurred image. We did a binary mask for now, but we are looking into fast methods of using proportional blur.</figcaption>
      </td>
    </tr>
  </table>
</div>
<video width="800px" height="600px" controls>
  <source src="images/zoom_1.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
<p>We then tried to these techniques with our own depth estimation</p>
<video width="800px" height="600px" controls>
  <source src="images/our_depth.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
<p>This is me going over the results we got so far in a more approachable format.</p>
<h2>Limitations</h2>
<h3>Objects in front of the plane of focus</h3>
<p>
  Since we estimate depth with how much region is out of focus, objects out of
  focus in front of the plane of focus are estimated to have great depth - there
  is an implicit assumption that everything in the scene is behind the plane of
  focus and there is no way to distinguish if something is in front of or
  behind the plane of focus.
</p>
<div>
<table>
  <tr valign="top">
    <td >
      <img src="images/limitations/photo1.jpeg" width="400px" />
      <figcaption >Original Photo</figcaption>
    </td>
    <td >
      <img src="images/limitations/iphoto1.png" width="400px" />
      <figcaption >Defocus Map</figcaption>
    </td>
  </tr>
</table>
</div>
<p>
  Here the petal at the bottom left is estimated to be the the closest part of
  the scene to the camera, although from the photo it is clear that this is not
  true. This is because the plane of focus is on that petal, and petals closer to the
  camera are rendered out of focus, and estimated to be further in the scene.
</p>
<h3>Color correlation</h3>
<p>
  The depth of an object in the scene should be independent of what color it is.
  However, defocus estimation ends up being affected by the colors anyway. The
  distribution of color around the image appears to affect the gradients of the
  image. This affects both the gradient comparison step and the interpolation step using the matting Laplacian, which factors RGB difference into pixel difference.
</p>
<div>
<table>
  <tr valign="top">
    <td >
      <img src="images/limitations/camera.png" width="400px" />
      <figcaption >Original Photo</figcaption>
    </td>
    <td >
      <img src="images/limitations/icamera.png" width="400px" />
      <figcaption >Defocus Map</figcaption>
    </td>
  </tr>
</table>
</div>
<p>
  In this example the silver lens is estimated to be very close to the camera
  due to the strong gradient given by the silver to black edge.
</p>
<div>
<table>
  <tr valign="top">
    <td >
      <img src="images/limitations/yqsmall.png" width="400px" />
      <figcaption >Original Photo</figcaption>
    </td>
    <td >
      <img src="images/limitations/iyq.png" width="400px" />
      <figcaption >Defocus Map</figcaption>
    </td>
  </tr>
</table>
<p>
  Again, there is a correlation between color and depth. It is a little more difficult
  to intepret what is going on in this portrait of our friend Yongqi. Focusing
  on just his face, we can see that even his face is not evenly depth mapped,
  especially across regions where the lighting falls. Part of this can also be
  attributed to the numerical instability in dividing gradients.
</p>
<p>In our repository, we've included a Jupyter Notebook and a Python script containing our implementation.</p>
</div>

<h2>Challenges/lessons learned</h2>
<p>
  This project involved working with large sparse matrices - L, the matting Laplacian,
  and D, the diagonalized edge detections (for instance, the matrix D for our
  initial 800x600 sample image was 480000x480000 with only a few thousand nonzero
  values) as coefficients in a linear equation. Thus, it was more realistic for
  us to use an approximator than find the exact solution. One challenge we
  didn’t anticipate was that different approximators (np.linalg.solve,
  scipy.sparse.spsolve, and the Matlab mldivide operator) yielded different
  results, some of which were significantly inaccurate. We found that spsolve
  with 1000 iterations was the most reliable.
</p>
<p>
  We also observed that hyperparameters like kernel size, edge detection
  threshold, lambda, etc. heavily affected the accuracy of our depth map. For
  example, setting epsilon (regularization strength) too high in the matting
  Laplacian function, with an otherwise correct implementation, produced an
  incomprehensible depth map.
</p>

<h2>Final Thoughts</h2>
<p>With depth mapping a popular feature of many modern smartphones using fancy
  multi lens arrays and machine learning algorithms, we wanted to take a step
  back and explore monocular, model free methods of determining depth in an
  image. We contribute a Python implementation of monocular depth mapping
  through defocus map estimation, using the natural defocus of a lens to
  estimate depth. This method gives us reasonable results in certain situations,
  but is otherwise a poor depth mapping method in general.
</p>
<p>
  Working on this project has given us some more intuition for how much more
  intelligence is needed to effectively determine depth in an image,
  either through inference models found in machine learning, or through
  stereo views.
</p>
<h2>Contributions</h2>
<h3>Kevin</h3>
<p>Experimented with enhanced image gradient method and implemented the defocus map estimation method</p>
<h3>Angela</h3>
<p>Worked on sparse to full interpolation step (implementing L, D and finding good solver), helped tune, clean, format and convert to script</p>
<h3>Ani</h3>
<p>Worked on effects (portrait mode, bokeh effects, and 3d ken burns).
</p>
</div>
</body>
</html>


<h2>References</h2>
<ol>
  <li>http://graphics.stanford.edu/papers/portrait/wadhwa-portrait-sig18.pdf - Google portrait mode paper
  <li>https://www.sciencedirect.com/science/article/pii/S1877050915031968 - Enhanced Image Gradients
  <li>https://www.comp.nus.edu.sg/~tsim/documents/defocusEstimation-published.pdf - Defocus map estimation from a single image
  <li>https://arxiv.org/pdf/1909.05483v1.pdf - 3d ken burns
  <li>https://arxiv.org/pdf/1810.08100.pdf - depth of field from single image
</ol>
