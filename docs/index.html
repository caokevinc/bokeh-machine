<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    padding: 100px;
    width: 800px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Lato', sans-serif;
    color: #121212;
    font-size: 16px;
    line-height: 1.5;
  }
  h1, h2, h3, h4 {
    font-family: 'Lato', sans-serif;
    text-align: left;
  }
  h1 {
    color: #002f4f;
    font-size: 30px;
    text-align: center;
    line-height: 1;
  }
  h2 {
    color: #2a5e82;
    font-size: 24px;
    margin-top: 40px;
  }
  h3 {
    color: #95b4c9;
    font-size: 20px;
    margin-top: 30px;
  }
  h4 {
    color: #3b3b3b;
    font-size: 16px;
    font-weight: 400;
    margin-top: 30px;
  }
  pre {
    padding: 0.75em 1em;
    background: #f4f4f4;
    border: solid 1px #eeeeee;
    border-radius: 4px;
    margin-bottom: 20px;
  }
  code {
    font-size: 85%;
    font-family: Monaco, monospace;
    color: #666666;
  }
  figcaption {
    color: #888888;
    font-size: 12px;
    font-style: italic;
    margin-top: 10px;
    margin-bottom: 10px;
    text-align: center;
  }
  img {
    display: block;
    margin-top: 10px;
    margin-left: auto;
    margin-right: auto;
  }
  table video {
    margin: 0;
  }
  video {
    margin-top: 10px;
    margin-bottom: 20px;
  }
  #bird-video {
    width: 400px;
    margin-top: 10px;
    margin-bottom: -6px;
  }
</style>
<title>CS 184 Final Project</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Lato:300,300i,400,400i,700,700i&display=swap" rel="stylesheet">
</head>

<body>
<h1>CS 184 Final Project Writeup</h1>
<h1>Bokeh Machine - Depth of Field Synthesizer</h1>
<h1>Kevin Cao, Angela Dong, Aniruddha Nrusimha </h1>

<h2>Abstract</h2>
<pre><code>testing code
sldkfj
testing code</code></pre>
<p>
  We were motivated by depth-of-field based visual effects like portrait mode (blurring based on depth) and Ken Burns (zooming based on depth). Given a singular image, these effects can be implemented with the help of a full depth map, which maps each pixel to a scalar indicating its depth value. Our main technical objective for this project was to calculate a depth map for an image from scratch.
</p>
<p>Examples of visual effects:</p>
<div>
  <table>
    <tr valign="top">
      <td align="middle">
        <img src="final-images/portrait.jpeg" width="375px" />
        <figcaption align="middle">Portrait mode</figcaption>
      </td>
      <td align="middle">
        <img src="final-images/kenburns.gif" width="400px" />
        <figcaption align="middle">Ken Burns effect</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>Example of a depth map:</p>
<img src="final-images/depthmap.jpg" width="700px" />
<figcaption align="middle">Sample depth map, with depth values ranging from 0 to 255; lighter color indicates closer</figcaption>
<p>
  After trying a few possible approaches, we decided to move forward with implementing an algorithm from the paper "Defocus map estimation from a single image". We were able to achieve visually similar depth maps to both the results from the original paper and depth maps generated from photography apps. Once our implementation was finished, we used off-the-shelf portrait mode and Ken Burns effect generators to apply these visual effects to our own images.
</p>

<h2>Technical Approach</h2>
<h3>Enhanced Image Gradients</h3>
<p>
  We first tried a method from
  <a href="https://www.sciencedirect.com/science/article/pii/S1877050915031968"> this paper</a>
  that tries to find a depth map with gradients close to the original image gradient.
  The assumption is that the gradients in the depth map should match up
  to the gradients in the image. The paper suggests and error function
  relating the image gradient to the depth map gradients and provides a
  closed form solution for the depth map.
</p>
<div>
  <table>
    <tr valign="top">
      <td align="middle">
        <img src="milestone-images/enhancedalgo.png" width="800px" />
        <figcaption align="middle">Algorithm for constructing Depth Map from Bala et. al</figcaption>
      </td>
  </table>
  <table>
    <tr valign="top">
      <td align="middle">
        <img src="milestone-images/yq.png" width="400px" />
        <figcaption align="middle">Original</figcaption>
      </td>
      <td align="middle">
        <img src="milestone-images/enhanced.png" width="400px" />
        <figcaption align="middle">Depth Map</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
  However we found that this method gave us a weak, and frankly horrifying
  depth map. This, at least in its current implementation, is a dead end for us.
</p>
<h3>Defocus map estimation</h3>
<div>
    <table>
      <tr valign="top">
        <td align="middle">
          <img src="milestone-images/edgeblur.png" width="800px" />
          <figcaption align="middle">Overview of blur estimation approach, taken from Zhuo et. al</figcaption>
        </td>
      </tr>
    </table>
</div>
<h4>Overview</h4>

<p>
    In this approach, we blur the image and take the ratio of the gradients
    of the original image with the gradients of the blurred image. This ratio
    gives us an estimation of the standard deviation of the original blur of a
    region of the image, as we model out of focus areas as a gaussian blur.
    This effectively gives us a "defocus map" which we can roughly equate to a
    depth map.
</p>
<h4>Method</h4>
<div>
  <table>
    <tr valign="top">
      <td align="middle">
        <img src="images/method/flower.png" width="200px" />
        <figcaption align="middle">Original Image</figcaption>
      </td>
    <tr>
    </table>
  </div>
<h3>Step 1: Gaussian Blur the Image</h3>
<p>
  We create two images. <br>
  i_1: original image with GaussianBlur of Standard Deviation 1 <br>
  i_2: original image with GaussianBlur of Standard Deviation 1.5 <br>
</p>
<div>
  <table>
    <tr valign="top">
      <td align="middle">
        <img src="images/method/og.png" width="320px" />
        <figcaption align="middle">i_1, Blurred with Standard Deviation 1</figcaption>
      </td>
      <td align="middle">
        <img src="images/method/blurry.png" width="320px" />
        <figcaption align="middle">i_2, Blurred with Standard Deviation 2</figcaption>
      </td>
    <tr>
    </table>
  </div>
<h3>Step 2: Take gradient of i_1 and i_2</h3>
<p>
  We use Sobel filters of kernel size 1 to take image gradients in the x and y
  direction to compute the magnitude of the full gradient.
</p>
<div>
  <table>
    <tr valign="top">
      <td align="middle">
        <img src="images/method/gradienteq.png" width="320px" />
        <figcaption align="middle">Magnitude of the gradient, where i_x and i_y are gradients in the x and y direction</figcaption>
      </td>
    <tr>
    </table>
  </div>
Python Implementation

<pre><code>  sobelx = cv2.Sobel(im, cv2.CV_64F, 1, 0, ksize=1)
  sobely = cv2.Sobel(im, cv2.CV_64F, 0, 1, ksize=1)
  grad = np.sqrt(sobelx**2 + sobely**2)</code></pre>
<p>
We use this to compute the gradients, gradient(i_1) and gradient(i_2).
</p>
<div>
  <table>
    <tr valign="top">
      <td align="middle">
        <img src="images/method/grad1.png" width="320px" />
        <figcaption align="middle">Gradient of i_1</figcaption>
      </td>
      <td align="middle">
        <img src="images/method/grad2.png" width="320px" />
        <figcaption align="middle">Gradient of i_2</figcaption>
      </td>
    </tr>
  </table>
</div>



<h3>Step 3: Use ratio of gradients to derive depth estimation</h3>
<p>We take the ratio of gradients R = Gradient(i_1)/Gradient(i_2)</p>

<div>
  <table>
    <tr valign="top">
      <td align="middle">
        <img src="images/method/diff.png" width="320px" />
        <figcaption align="middle">Map of Gradients</figcaption>
      </td>
    </tr>
    </table>
  </div>

  Derivation of Depth Estimation

  <div>
    <table>
      <tr valign="top">
        <td align="middle">
          <img src="images/method/r2.png" width="150px" />
          <figcaption align="middle"></figcaption>
        </td>
      </tr>
    </table>
  </div>
  <p>
    R is ratio of Gradients<br>
    S is scene depth<br>
    Sigma1 is Standard Deviation of Gaussian Blur applied to i_1<br>
    Sigma2 is Standard Deviation of Gaussian Blur applied to i_2<br>
    The intuition is that at infinite depth, or a completely out of focus region
    of the image, adding a gaussian blur does nothing, aka R = 1. At 0 depth, or
    a completely sharp region of the image, we expect R = sigma1/sigma2, in
    other words, only the gaussian blurs have caused the difference. With some
    algebra we are able to derive s, the estimated depth.
  </p>
    <div>
      <table>
        <tr valign="top">
          <td align="middle">
            <img src="images/method/s.png" width="150px" />
            <figcaption align="middle"></figcaption>
          </td>
        </tr>
      </table>
    </div>
    <div>
      <table>
        <tr valign="top">
          <td align="middle">
            <img src="images/method/sig.png" width="320px" />
            <figcaption align="middle">Estimated Depth Map</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <p>
      We notice that the depth estimation is strongest at edges. Defocus
      estimation is only useful at edges, since the blur affects edges high
      frequency areas the most.
    </p>

    <h3>Step 4: Use and Edge Map to create Sparse Defocus Map</h3>

    <p>We use a Canny Edge Detector to mask for only the edges in our Depth
      Estimation Map. This produces a sparse defocus map, where we have an
      estimate of depth at every edge.
    </p>
    <div>
      <table>
        <tr valign="top">
          <td align="middle">
            <img src="images/method/sig.png" width="250px" />
            <figcaption align="middle">Estimated Depth Map</figcaption>
          </td>
          <td align="middle">
            <img src="images/method/star.png" width="80px" />
          </td>
          <td align="middle">
            <img src="images/method/edgemap.png" width="250px" />
            <figcaption align="middle">Edge Map</figcaption>
          </td>
          <td align="middle">
            <img src="images/method/equal.png" width="75px" />
          </td>
          <td align="middle">
            <img src="images/method/sparse.png" width="250px" />
            <figcaption align="middle">Sparse Defocus Map</figcaption>
          </td>
        </tr>
      </table>
    </div>
<h3>Step 5: Matting Laplacian</h3>
<p>TODO</p>
<h3>Step 6. (Optional) Increase contrast</h3>
<p>
  Sometimes we get flat depth maps. To increase the spread of depth,
  we can use a Gamma Correction to increase the contrast.
</p>
<div>
  <table>
    <tr valign="top">
      <td align="middle">
        <img src="images/method/gamma.png" width="100px" />
        <figcaption align="middle">Gamma Correction</figcaption>
      </td>
    </tr>
  </table>
</div>
<div>
<table>
  <tr valign="top">
    <td align="middle">
      <img src="images/method/iflower.png" width="250px" />
      <figcaption align="middle">Raw Defocus Map</figcaption>
    </td>
    <td align="middle">
      <img src="images/method/iflowercontrast.png" width="250px" />
      <figcaption align="middle">Added Contrast Defocus Map</figcaption>
    </td>
  </tr>
</table>
</div>



<p>
  We can see in the sparse defocus map that edges in the foreground are lighter than
  those in the background. In this visualization, this means that the edges
  in the foreground have original blurs less than those in the background,
  and is a hopeful sign that we are on the right track.
</p>
<p> INSERT MORE DETAIL ON FILLING IN SPARSE MAP </p>
<p> Using the constructed depth map, we get strong results on a variety of images.  As reference, lighter portions of the image are closer to the camera </p>
<p> These results also allow us to construct visual effects on our results </p>
<div>
  <table>
    <tr>
      <td>
        <img src="images/input.png" width="250px"/>
        <figcaption>Original image</figcaption>
      </td>
      <td>
        <img src="images/bird_depth.png" width="250px"/>
        <figcaption>Depth we generated</figcaption>
      </td>
      <td>
        <img src="images/binary_blur.png" width="250px"/>
        <figcaption>Foreground - background extraction</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>We can use these depth extractions for cool effects, which we will discuss later.</p>
<div>
  <table>
    <tr>
      <td>
        <img src="images/super_blur.png" width="400px"/>
        <figcaption>Enhancing bokeh effects for the picture</figcaption>
      </td>
      <td>
        <video id="bird-video" controls>
          <source src="images/bird.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <figcaption>Ken Burns effect</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>We tested our depth map estimation on a variety of images, and got generally good results.  For a tough scenario, We took some pictures of one of flowers, which are highly detailed and complex depth maps. </p>
<div>
  <table>
    <tr>
      <td>
        <img src="images/examples/photo1.jpeg" width="400px"/>
        <figcaption>First picture of flowers</figcaption>
      </td>
      <td>
        <img src="images/examples/photo1d.png" width="400px"/>
        <figcaption>Depth picture</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/examples/photo2.jpeg" width="400px"/>
        <figcaption>Second pictures of flowers</figcaption>
      </td>
      <td>
        <img src="images/examples/photo2d.png" width="400px"/>
        <figcaption>Depth picture</figcaption>
      </td>
    </tr>
    <tr>
      <td>
        <img src="images/examples/photo3.jpeg" width="400px"/>
        <figcaption>Third picture of flowers</figcaption>
      </td>
      <td>
        <img src="images/examples/photo3d.png" width="400px"/>
        <figcaption>Depth picture</figcaption>
      </td>
    </tr>
  </table>
</div>

<h2>Results</h2>
<h3>Out of the box depth estimation and effects </h3>
<p> We also used out of the box depth estimation to test out some cool effects.  We listed two we implemented below </p>
<div>
  <table>
    <tr>
      <td>
        <img src="images/rclouds.png" width="400px"/>
        <figcaption>Original Image</figcaption>
      </td>
      <td>
        <img src="images/depth.jpg" width="400px"/>
        <figcaption>Depth mapping</figcaption>
      </td>
    </tr>
  </table>
  <table>
    <tr>
      <td>
        3D Ken Burns Effect
      </td>
      <td>
        Elements which are closer to the camera are zoomed in more than those farther away, simulating the camera moving closer to the image.
        In addition, blown up closer versions of the image are laid over the images farther back.  We mostly used an out of the box implementation for this part.
      </td>
      <td>
        <video width="400px" controls>
          <source src="images/autozoom.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>
    </tr>
    <tr>
      <td>
        Artificial Lens Blur
      </td>
      <td>
        We establish a focus point in the depth map.  Because we only had relative depth measurements for depth, we assumed the image was focused at infinity.
        We then simulated a thin lens effect by picking a point as our point of focus, and estimating the degree of blur using lens equations learned in class.
        This technique is meant for cameras focused at infinity, and is effectively the 'inverse' of the idea of the defocus map - given a depth map, determine blur.
      </td>
      <td>
        <img src="images/portrait.png" width="400px"/>
        <figcaption>partially blurred image. We did a binary mask for now, but we are looking into fast methods of using proportional blur.</figcaption>
      </td>
    </tr>
  </table>
</div>
<video width="800px" height="600px" controls>
  <source src="images/zoom_1.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
<p>We then tried to these techniques with our own depth estimation</p>
<video width="800px" height="600px" controls>
  <source src="images/our_depth.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
<p>This is me going over the results we got so far in a more approachable format.</p>
<h2>Limitations</h2>
<h3>Objects in Front of the plane of focus</h3>
<p>
  Since we estimate depth with how much region is out of focus, objects out of
  focus in front of the plane of focus are estimated to have great depth - there
  is an implicit assumption that everything in the scene is behind the plane of
  focus and there is no way to distinguish if something is in front of or
  behind the plane of focus.
</p>
<div>
<table>
  <tr valign="top">
    <td align="middle">
      <img src="images/limitations/photo1.jpeg" width="400px" />
      <figcaption align="middle">Original Photo</figcaption>
    </td>
    <td align="middle">
      <img src="images/limitations/iphoto1.png" width="400px" />
      <figcaption align="middle">Defocus Map</figcaption>
    </td>
  </tr>
</table>
</div>
<p>
  Here the petal at the bottom left is estimated to be the the closest part of
  the scene to the camera, although from the photo it is clear that this is not
  true. This is because the plane of focus is on that petal, and petals closer to the
  camera are rendered out of focus, and estimated to be further in the scene.
</p>
<h3>Color Correlation</h3>
<p>
  The depth of an object in the scene should be independent of what color it is.
  However defocus estimation ends up being affected by the colors anyway. The
  distribution of color around the image appears to affect the gradients of the
  image.
</p>
<div>
<table>
  <tr valign="top">
    <td align="middle">
      <img src="images/limitations/camera.png" width="400px" />
      <figcaption align="middle">Original Photo</figcaption>
    </td>
    <td align="middle">
      <img src="images/limitations/icamera.png" width="400px" />
      <figcaption align="middle">Defocus Map</figcaption>
    </td>
  </tr>
</table>
</div>
<p>
  In this example the silver lens is estimated to be very close to the camera
  due to the strong gradient given by the silver to black edge.
</p>
<div>
<table>
  <tr valign="top">
    <td align="middle">
      <img src="images/limitations/yqsmall.png" width="400px" />
      <figcaption align="middle">Original Photo</figcaption>
    </td>
    <td align="middle">
      <img src="images/limitations/iyq.png" width="400px" />
      <figcaption align="middle">Defocus Map</figcaption>
    </td>
  </tr>
</table>
<p>
  Again, there is a correlation between color and depth. It is a little more difficult
  to intepret what is going on in this portrait of our friend Yongqi. Focusing
  on just his face, we can see that even his face is not evenly depth mapped,
  especially across regions where the lighting falls. Part of this can also be
  attributed to the numerical instability in dividing gradients. 
</p>
</div>

<h2>References</h2>
<ol>
  <li>http://graphics.stanford.edu/papers/portrait/wadhwa-portrait-sig18.pdf - Google portrait mode paper
  <li>https://www.sciencedirect.com/science/article/pii/S1877050915031968 - Enhanced Image Gradients
  <li>https://www.comp.nus.edu.sg/~tsim/documents/defocusEstimation-published.pdf - Defocus map estimation from a single image
  <li>https://arxiv.org/pdf/1909.05483v1.pdf - 3d ken burns
  <li>https://arxiv.org/pdf/1810.08100.pdf - depth of field from single image
</ol>

<h2>Contributions</h2>
<h3>Kevin</h3>
<p></p>
<h3>Angela</h3>
<p></p>
<h3>Ani</h3>
<p></p>
</div>
</body>
</html>
